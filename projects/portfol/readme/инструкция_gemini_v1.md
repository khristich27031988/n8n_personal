# Инструкция по текущему состоянию Workflow автоматизации аналитики (n8n)

## 1. Введение и Постановка Задачи

**Цель проекта:** Создать автоматизированный workflow в n8n для анализа бизнес-данных, загружаемых пользователем в формате CSV. Система должна помогать в подготовке данных к анализу, предлагать возможные направления исследования и, в перспективе, выполнять сам анализ и генерацию инсайтов.

**Входные данные:**
*   Три JSON-файла с описанием отдельных workflows n8n (`csv_analyzer.json`, `data_transformer.json`, `insights_generator.json`).
*   Markdown-файл с документацией проекта (`ПРОЕКТ_ДОКУМЕНТАЦИЯ.md`).
*   Пользователь взаимодействует с системой через чат-интерфейс n8n (`n8n Agent Chat`).
*   Пользователь прикрепляет CSV-файл к сообщению в чате.
*   Система использует LLM (предположительно Qwen API) для интеллектуального анализа.
*   n8n развернут на VPS пользователя в Docker-контейнере.

**Первоначальная цель:** Объединить три предоставленных workflow в один сквозной процесс: Загрузка -> Анализ LLM (структура, рекомендации) -> Трансформация -> Сохранение в SQLite -> Генерация инсайтов LLM.

**Текущая (уточненная) цель первого этапа:** Создать часть workflow, которая:
1.  Принимает CSV-файл из чата.
2.  Извлекает метаданные файла и текст запроса пользователя.
3.  Анализирует структуру CSV (заголовки, типы данных на основе сэмпла).
4.  **Генерирует подробный текстовый промпт-задачу** для следующего этапа (AI-агента или LLM), описывающий, какие виды бизнес-анализа можно провести с этими данными.
5.  Передает этот промпт и исходные данные дальше.

**Предназначение:** Проект для портфолио и отработки навыков, с перспективой коммерческого использования.

## 2. Процесс Разработки: Сложности и Решения

Наш путь к текущему состоянию был итеративным и включал решение ряда технических сложностей, связанных с особенностями работы n8n, особенно с Python-узлами и передачей бинарных данных:

1.  **Выбор триггера:** Перешли от `Webhook` к `When chat message received` для интерактивности.
2.  **Доступ к файлу из чата:**
    *   **Проблема:** Первоначальные попытки получить доступ к бинарным данным файла из Python-узла (используя предполагаемые пути `item.binary.data` или атрибут `id`) не увенчались успехом (`AttributeError`, файл не найден).
    *   **Диагностика:** Анализ реальных выходных данных триггера показал, что метаданные файла находятся в `item.json.files`, а бинарные данные привязаны к элементу под ключом, обычно `'data0'`.
    *   **Проблема JsProxy:** При попытке доступа к `item.json.files` из Python столкнулись с тем, что n8n передает не нативные Python-словари/списки, а объекты `pyodide.ffi.JsProxy`. Это мешало стандартным проверкам типа `isinstance(..., list)`.
    *   **Решение:** Использование метода `.to_py()` для явного преобразования `JsProxy` в Python-объекты перед дальнейшей обработкой. Это позволило корректно извлекать метаданные (`fileName`, `userQuestion`) из `item.json`.
3.  **Передача бинарных данных между узлами:**
    *   **Проблема:** Попытки передать бинарные данные (байты) от первого Python-узла ко второму (для парсинга) оказались нестабильными. Передача "ссылки" (имени ключа) не всегда работала, а попытка получить байты через `item.get_binary_data()` во втором узле часто приводила к ошибкам (`RuntimeError: Failed to get binary data... Error: get_binary_data`), даже если данные присутствовали.
    *   **Решение:** Использование **встроенного узла n8n** для обработки CSV. Узел `Extract From CSV` оказался способен напрямую работать с бинарными данными (`data0`), переданными от первого Python-узла, и парсить их в N элементов (строк). Это устранило необходимость в сложных манипуляциях с байтами внутри Python-кода на этапе парсинга.
4.  **Объединение данных для промпта:**
    *   **Проблема:** Нам нужно было собрать метаданные (от Узла 1) и сэмпл данных (от Узла 2) для формирования промпта в Python-узле.
    *   **Попытка 1 (Merge Append/Combine):** Узел `Merge` в режимах `Append` или `Combine By Position/Index` не дал нужного результата – он либо добавлял метаданные после всех строк, либо объединял их только с первой строкой.
    *   **Попытка 2 (Python с 2 входами):** Попытка подключить оба предыдущих узла напрямую к Python-узлу не работала корректно в режиме тестирования (`Test step`), так как тест не симулировал одновременное поступление данных с двух входов.
    *   **Решение (Текущее):** Использование цепочки:
        *   `Extract From CSV` (выдает N строк).
        *   `SplitInBatches` (берет первые 5 строк и выдает **1 элемент** со списком этих строк).
        *   `Merge` (режим `Merge By Index`, объединяет **1 элемент** метаданных и **1 элемент** со списком строк в **1 итоговый элемент**).
        *   `Prepare Analysis Prompt` (Python, получает **1 элемент** от Merge и легко работает с ним).
    *   **Дополнительная проблема JsProxy:** Даже после `Merge` значение под ключом со списком строк (например, `"sample"`) все еще могло быть `JsProxy`. В итоговый код Python-узла добавлена явная проверка и конвертация `.to_py()` для этого списка.
5.  **Уточнение цели Python-узла:** Изначально предполагалось, что Python-узел будет *запрашивать* анализ у LLM. По вашему уточнению, его цель изменилась на **генерацию текстового промпта-задачи** для следующего этапа анализа. Код был соответствующим образом переписан.

## 3. Текущее Состояние Workflow (На April 6, 2025)

**Схема:**
Use code with caution.
Markdown
[When chat message received] (Триггер)
|
+--> [Extract Params (Python)] (Узел 1) -----> [Merge (Mode: Merge By Index)] (Узел 4) ---> [Prepare Analysis Prompt (Python)] (Узел 5) -> (ДАЛЬШЕ)
| ^
+--> [Extract From CSV (Встроенный)] (Узел 2) -> [SplitInBatches (Size=5)] (Узел 3) ----------+
**Описание узлов:**

1.  **`When chat message received`:**
    *   Триггер, запускается при получении сообщения с прикрепленным файлом в чате n8n.
    *   Выдает 1 элемент, содержащий `item.json` (с `files`, `chatInput`) и `item.binary` (с `data0`).

2.  **`Extract Params` (Python):**
    *   Получает 1 элемент от триггера.
    *   Извлекает `fileName` (из `item.json.files[0]`), `userQuestion` (из `item.json.chatInput`).
    *   Генерирует `fileId`.
    *   **Выход:** 1 элемент JSON `{ "fileId": ..., "fileName": ..., "userQuestion": ... }` и привязанные бинарные данные `'data0'` (через `binary: {'data0': 'data0'}`).

3.  **`Extract From CSV` (Встроенный):**
    *   Получает 1 элемент от `Extract Params` (включая бинарные данные `data0`).
    *   **Operation:** `Extract From CSV`.
    *   **Input Binary Field:** `data0`.
    *   **Выход:** N элементов, где N - количество строк в CSV. Каждый элемент - JSON, представляющий одну строку (`{ "Col A": "val1", "Col B": "val2", ... }`).

4.  **`SplitInBatches` (Встроенный):**
    *   Получает N элементов от `Extract From CSV`.
    *   **Batch Size:** `5`.
    *   **Выход:** **1 элемент**. Его JSON содержит список (`list`) первых 5 элементов (первых 5 строк CSV в формате JSON). Ключ этого списка по умолчанию обычно `"items"` или `"data"`. (Мы предположили, что Merge назовет его "sample" на следующем шаге, но это нужно проверять на выходе Merge).

5.  **`Merge` (Встроенный):**
    *   **Mode:** `Merge By Index`.
    *   **Input 1:** От `Extract Params` (1 элемент с метаданными).
    *   **Input 2:** От `SplitInBatches` (1 элемент со списком первых 5 строк).
    *   **Выход:** **1 элемент**. Его JSON содержит поля из обоих входов, объединенные на верхнем уровне (например, `{ "fileId": ..., "fileName": ..., "userQuestion": ..., "sample": [ {строка1}, {строка2}, ... ] }`). Бинарные данные `data0` от Input 1 также передаются дальше.

6.  **`Prepare Analysis Prompt` (Python):**
    *   Получает 1 элемент от `Merge`.
    *   Извлекает `fileId`, `fileName`, `userQuestion`.
    *   Извлекает список `sample_data` по ключу `"sample"` (с проверкой на JsProxy и конвертацией `.to_py()`).
    *   Определяет заголовки (`headers`) и базовые типы (`columnTypes`) на основе `sample_data`.
    *   **Генерирует текстовый промпт (`analysis_prompt`)**, описывающий возможные направления бизнес-анализа на основе этих данных.
    *   **Выход:** **1 элемент** JSON `{ "fileId": ..., "fileName": ..., "userQuestion": ..., "analysis_prompt": "...", "headers": ..., "columnTypes": ..., "sampleData": ... }` и привязанные бинарные данные `'data0'`.

**Что сделано:** Мы успешно настроили цепочку узлов, которая принимает CSV из чата, извлекает метаданные, парсит первые 5 строк и генерирует текстовый промпт-задачу для дальнейшего анализа. Мы преодолели сложности с передачей данных и использованием Python-узлов.

**Что хотим сделать дальше:**

1.  **Использовать `analysis_prompt`:** Передать сгенерированный `analysis_prompt` следующему узлу. Этот узел может быть:
    *   Другим **HTTP Request** к LLM (Qwen API), который уже будет выполнять сам бизнес-анализ, руководствуясь этой задачей.
    *   Специализированным **AI Агентом** (если он у вас настроен).
    *   Просто узлом **Set** или **Respond to Webhook**, если вы хотите просто *увидеть* сгенерированный промпт на этом этапе.
2.  **Обработка ответа от LLM/Агента:** Добавить узлы для получения и обработки ответа с результатами бизнес-анализа (инсайты, рекомендации).
3.  **Реализация трансформации данных:** Добавить узлы (вероятно, Python), которые будут получать исходные бинарные данные (`data0`) и список *рекомендуемых* трансформаций (полученный от LLM на *первом* этапе анализа структуры, если мы вернемся к этой идее, или заданный пользователем), применять их и сохранять результат (например, в SQLite, как планировалось изначально).
4.  **Визуализация и взаимодействие:** Интеграция с Streamlit для отображения результатов анализа, инсайтов и, возможно, управления процессом.

На данный момент у нас готов этап **генерации задачи для бизнес-анализа**.